# Developed and fine-tuned a 1.1B parameter TinyLlama language model using Low-Rank Adaptation (LoRA) techniques to efficiently adapt the model for mathematical reasoning tasks on the GSM8K dataset.
# Engineered a robust data preprocessing and tokenization pipeline for both standard (GSM8K) and custom (Frobinate) instruction-response datasets, enabling seamless model adaptation to new domains.
# Implemented quantization-aware training with 4-bit precision (using BitsAndBytes) to optimize model memory usage and accelerate training on limited hardware resources.
# Designed and executed comprehensive evaluation protocols, including perplexity measurement and qualitative output comparison, to benchmark model performance on both seen and unseen data.
# Automated the saving, loading, and merging of LoRA adapters, facilitating modular deployment and rapid experimentation with different task-specific adapters.
